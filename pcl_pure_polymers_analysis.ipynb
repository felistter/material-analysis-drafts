{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws/ih7618/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:913: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import ast\n",
    "import itertools\n",
    "from StringIO import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, filters, measure, feature, exposure, color, morphology, draw\n",
    "from skimage.feature import register_translation\n",
    "from skimage import transform as tf\n",
    "import fabio as fb\n",
    "import mahotas as mh\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "import vigra\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "_MEASUREMENTS = {\n",
    "    'Label': 'label',\n",
    "    'Area': 'area',\n",
    "    'Perimeter': 'perimeter'\n",
    "}\n",
    "\n",
    "_MEASUREMENTS_VALS = _MEASUREMENTS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/zenr/ippy\n",
    "\n",
    "\"\"\"\n",
    "Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "Usage: $ python max_entropy.py <gray scale image>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def max_entropy(data):\n",
    "    \"\"\"\n",
    "    Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "    Kapur J.N., Sahoo P.K., and Wong A.K.C. (1985) \"A New Method for Gray-Level Picture Thresholding Using the Entropy\n",
    "    of the Histogram\", Graphical Models and Image Processing, 29(3): 273-285\n",
    "    M. Emre Celebi\n",
    "    06.15.2007\n",
    "    Ported to ImageJ plugin by G.Landini from E Celebi's fourier_0.8 routines\n",
    "    2016-04-28: Adapted for Python 2.7 by Robert Metchev from Java source of MaxEntropy() in the Autothresholder plugin\n",
    "    http://rsb.info.nih.gov/ij/plugins/download/AutoThresholder.java\n",
    "    :param data: Sequence representing the histogram of the image\n",
    "    :return threshold: Resulting maximum entropy threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate CDF (cumulative density function)\n",
    "    cdf = data.astype(np.float).cumsum()\n",
    "\n",
    "    # find histogram's nonzero area\n",
    "    valid_idx = np.nonzero(data)[0]\n",
    "    first_bin = valid_idx[0]\n",
    "    last_bin = valid_idx[-1]\n",
    "\n",
    "    # initialize search for maximum\n",
    "    max_ent, threshold = 0, 0\n",
    "\n",
    "    for it in range(first_bin, last_bin + 1):\n",
    "        # Background (dark)\n",
    "        hist_range = data[:it + 1]\n",
    "        hist_range = hist_range[hist_range != 0] / cdf[it]  # normalize within selected range & remove all 0 elements\n",
    "        tot_ent = -np.sum(hist_range * np.log(hist_range))  # background entropy\n",
    "\n",
    "        # Foreground/Object (bright)\n",
    "        hist_range = data[it + 1:]\n",
    "        # normalize within selected range & remove all 0 elements\n",
    "        hist_range = hist_range[hist_range != 0] / (cdf[last_bin] - cdf[it])\n",
    "        tot_ent -= np.sum(hist_range * np.log(hist_range))  # accumulate object entropy\n",
    "\n",
    "        # find max\n",
    "        if tot_ent > max_ent:\n",
    "            max_ent, threshold = tot_ent, it\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_data(filepath):\n",
    "    _, glob_ext = os.path.splitext(os.path.basename(filepath))\n",
    "    data = None\n",
    "    \n",
    "    if os.path.isdir(filepath):\n",
    "        data = np.asarray([io.imread(os.path.join(filepath, fp)) for fp in sorted(os.listdir(filepath))])\n",
    "        return data\n",
    "\n",
    "    if glob_ext == '.raw':\n",
    "        name, bits, size, ext = parse_filename(filepath)\n",
    "        data_type = np.float32 if bits == 32 else np.uint8\n",
    "        data = np.memmap(filepath, dtype=data_type, shape=tuple(reversed(size)))\n",
    "    else:\n",
    "        print 'Incorrent file format, or filename.'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_filename(filepath):\n",
    "    basename, ext = os.path.splitext(os.path.basename(filepath))\n",
    "\n",
    "    comps = basename.split('_')\n",
    "    size = tuple([int(v) for v in comps[-1:][0].split('x')])\n",
    "    bits = int(re.findall('\\d+', comps[-2:-1][0])[0])\n",
    "    name = '_'.join(comps[:-2])\n",
    "\n",
    "    return name, bits, size, ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_gdocs(url):\n",
    "    import requests\n",
    "    r = requests.get(url)\n",
    "    data = r.content\n",
    "    df = pd.read_csv(StringIO(data))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_samples_configs(df):\n",
    "    columns = {'Offsets (start, end)': 'offset', \n",
    "               'Slice bounding box(x, y, w, h)': 'bbox', \n",
    "               'Analysis': 'analysis_type', \n",
    "               'Type': 'structure', \n",
    "               'Radius': 'radius', \n",
    "               'Slice folder': 'slice_folder',\n",
    "               'Particles density': 'density', \n",
    "               'Center': 'center', \n",
    "               'Priority': 'priority'}\n",
    "\n",
    "    cols = df.columns[df.columns.isin(columns.keys())]\n",
    "    \n",
    "    out = dict.fromkeys(df['Name'].values, None)\n",
    "    for name in df['Name'].values:\n",
    "        out[name] = df[df['Name'] == name][cols].to_dict(orient='records')\n",
    "        out[name] = {columns[k]: tryeval(v) for k, v in out[name][0].items()}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_raw_data(samples, datasets_dir, output_dir, roi=((-225,225),(-300,300),(-300,300))):\n",
    "    for sample_name, params in samples.iteritems():\n",
    "        filepath = [os.path.join(datasets_dir, fn) for fn in os.listdir(datasets_dir) if '{}_'.format(sample_name) in fn][0]\n",
    "        \n",
    "        data = np.memmap(filepath, dtype=np.float32, shape=params['shape'], mode='r')\n",
    "        cz, cy, cx = [v/2 for v in data.shape]\n",
    "        side_sizes = [np.sum(np.abs(r)) for r in roi]\n",
    "        \n",
    "        print cz, cy, cx\n",
    "        \n",
    "        cropped_data = data[slice(cz+roi[0][0], cz+roi[0][1]),\n",
    "                            slice(cy+roi[1][0], cy+roi[1][1]),\n",
    "                            slice(cx+roi[2][0], cx+roi[2][1])]\n",
    "        \n",
    "        ofld_name = '{}_{}x{}x{}'.format(sample_name, side_sizes[2], side_sizes[1], side_sizes[0])\n",
    "        odir = os.path.join(output_dir, ofld_name, 'slices')\n",
    "        \n",
    "        if not os.path.exists(odir):\n",
    "            os.makedirs(odir)\n",
    "        \n",
    "        for i, dslice in enumerate(cropped_data):\n",
    "            io.imsave(os.path.join(odir, 'slice_{:04}.tif'.format(i)), dslice)\n",
    "            \n",
    "            if (i % 100 == 0) or (i == cropped_data.shape[0] - 1):\n",
    "                print '{}/{}'.format(i, cropped_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_particles(data, min_eccentricity=0.98):\n",
    "    lbl = ndi.measurements.label(data)[0]\n",
    "    regions = measure.regionprops(lbl)\n",
    "        \n",
    "    for region in regions:\n",
    "        circ = (4. * np.pi * region.area) / (region.perimeter ** 2)\n",
    "\n",
    "        if region.eccentricity > min_eccentricity:\n",
    "            data[lbl == region.label] = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_slice(slice_mask, non_zeros_ratio=0.8):\n",
    "    dilated_slice_mask = ndi.morphology.binary_dilation(slice_mask, iterations=3).flatten()\n",
    "    num_nonzeros = np.count_nonzero(dilated_slice_mask)\n",
    "\n",
    "    if num_nonzeros > dilated_slice_mask.size * non_zeros_ratio:\n",
    "        return np.zeros_like(slice_mask)\n",
    "    \n",
    "    return slice_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_sample(samples, input_dir, roi=((-225,225),(-300,300),(-300,300)), \n",
    "                   check_slices=True, gauss_sigma=1, fibers_md_rad=0, min_eccentricity=0.98):\n",
    "    for sample_name, configs in samples.iteritems():\n",
    "        start = timer()\n",
    "        \n",
    "        print 'Sample #' + sample_name\n",
    "\n",
    "        components = configs['types']\n",
    "        rng = configs.get('range')\n",
    "        \n",
    "        print components\n",
    "        \n",
    "        sample_dir = [os.path.join(input_dir, fn) for fn in os.listdir(input_dir) if sample_name in fn][0]\n",
    "        slices_dir = os.path.join(sample_dir, 'slices')\n",
    "        \n",
    "        for i, comp in enumerate(components):\n",
    "            slice_paths = sorted([os.path.join(slices_dir, fn) for fn in os.listdir(slices_dir)])\n",
    "            slice_paths = slice_paths[rng] if rng is not None else slice_paths\n",
    "            \n",
    "            for fpath in slice_paths:\n",
    "                cropped_data = io.imread(fpath)\n",
    "                filename = os.path.basename(fpath)\n",
    "\n",
    "                p2 = np.percentile(cropped_data, 0.01)\n",
    "                p98 = np.percentile(cropped_data, 99.99)\n",
    "        \n",
    "                cropped_data = ndi.filters.gaussian_filter(cropped_data, gauss_sigma)\n",
    "                cropped_data = exposure.rescale_intensity(cropped_data, in_range=(p2, p98))\n",
    "                data_8bit = exposure.rescale_intensity(cropped_data, in_range='image', out_range=np.uint8).astype(np.uint8)\n",
    "                \n",
    "                if comp == 'particles':\n",
    "                    th = max_entropy(np.histogram(data_8bit, bins=256, range=(0, 256))[0])\n",
    "                    mask = (data_8bit >= th).astype(np.uint8)\n",
    "                    if check_slices:\n",
    "                        mask = check_slice(mask)\n",
    "                    mask = ndi.filters.median_filter(mask, size=2)\n",
    "                    mask = filter_particles(mask, min_eccentricity=min_eccentricity)\n",
    "                    \n",
    "                if comp == 'fibers':\n",
    "                    th = mh.otsu(data_8bit, ignore_zeros=True)\n",
    "                    mask = (data_8bit >= th).astype(np.uint8)\n",
    "                    if fibers_md_rad:\n",
    "                        mask = ndi.filters.median_filter(mask, size=fibers_md_rad)\n",
    "                    if check_slices:\n",
    "                        mask = check_slice(mask)\n",
    "                    #mask = ndi.morphology.binary_closing(mask, iterations=2)\n",
    "                    #mask = ndi.morphology.binary_fill_holes(mask)\n",
    "                    \n",
    "                if comp == 'fibers_no_particles':\n",
    "                    th_particles = max_entropy(np.histogram(data_8bit, bins=256, range=(0, 256))[0])\n",
    "                    th_fibers = mh.otsu(data_8bit, ignore_zeros=True)\n",
    "                    \n",
    "                    mask_particles = (data_8bit >= th_particles).astype(np.uint8)\n",
    "                    mask_fibers = (data_8bit >= th_fibers).astype(np.uint8)\n",
    "                    \n",
    "                    if check_slices:\n",
    "                        mask_particles = check_slice(mask_particles)\n",
    "                        mask_fibers = check_slice(mask_fibers)\n",
    "                    \n",
    "                    if not np.count_nonzero(mask_particles * mask_fibers):\n",
    "                        mask = np.zeros_like(mask_fibers)\n",
    "                    else:\n",
    "                        mask = mask_fibers.astype(np.int8) - ndi.morphology.binary_dilation(mask_particles, \n",
    "                                                                            structure=morphology.disk(2),\n",
    "                                                                            iterations=2).astype(np.int8)\n",
    "                        mask[mask < 0] = 0\n",
    "                    \n",
    "                out_dir = os.path.join(sample_dir, comp)\n",
    "                  \n",
    "                if not os.path.exists(out_dir):\n",
    "                    os.makedirs(out_dir)\n",
    "                    \n",
    "                io.imsave(os.path.join(out_dir, filename), mask.astype(np.uint8))\n",
    "            \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def raw2slices(input_path, output_dir, out_fld='slices', file_fmt='slice_{}.tif'):\n",
    "    name, bits, size, ext = parse_filename(input_path)\n",
    "    data = open_data(input_path)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, name, out_fld)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    print input_path\n",
    "    \n",
    "    for idx, _slice in enumerate(data):\n",
    "        if idx % 100 == 0 or idx == data.shape[0]-1:\n",
    "            print '{}/{}'.format(idx, data.shape[0]-1)\n",
    "            \n",
    "        io.imsave(os.path.join(output_path, file_fmt.format(idx)), _slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment samples for porosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#'rngs': {'particles':[0,185]}\n",
    "por_samples = {'sample21': {'types': ['particles', 'fibers', 'fibers_no_particles']},\n",
    "               'sample22': {'types': ['particles', 'fibers', 'fibers_no_particles']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #sample21\n",
      "['particles', 'fibers', 'fibers_no_particles']\n",
      "575.89292717\n",
      "Sample #sample22\n",
      "['particles', 'fibers', 'fibers_no_particles']\n",
      "597.208711863\n"
     ]
    }
   ],
   "source": [
    "segment_sample(por_samples, './data/samples/porosity', \n",
    "               check_slices=True,\n",
    "               min_eccentricity=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MEASUREMENTS = {\n",
    "    'Label': 'label',\n",
    "    'Area': 'area',\n",
    "    'Perimeter': 'perimeter'\n",
    "}\n",
    "\n",
    "_MEASUREMENTS_VALS = _MEASUREMENTS.values()\n",
    "\n",
    "def object_counter(stack_binary_data):\n",
    "    print 'Object counting - Labeling...'\n",
    "    labeled_stack, num_labels = ndi.measurements.label(stack_binary_data)\n",
    "    objects_stats = pd.DataFrame(columns=_MEASUREMENTS_VALS)\n",
    "    \n",
    "    print 'Object counting - Stats gathering...'\n",
    "    for slice_idx in np.arange(labeled_stack.shape[0]):\n",
    "        for region in measure.regionprops(labeled_stack[slice_idx]):\n",
    "            objects_stats = objects_stats.append({_measure: region[_measure] \\\n",
    "                                        for _measure in _MEASUREMENTS_VALS}, \\\n",
    "                                            ignore_index=True)\n",
    "            \n",
    "    print 'Object counting - Stats grouping...'\n",
    "    objects_stats = objects_stats.groupby('label', as_index=False).sum()\n",
    "\n",
    "    return objects_stats, labeled_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_particles(samples, input_dir):\n",
    "    for sample_name, configs in samples.items():\n",
    "        components = configs['types']\n",
    "        \n",
    "        if 'particles' in components:\n",
    "            print '###### Processing of {}'.format(sample_name)\n",
    "            \n",
    "            sample_dir = [os.path.join(input_dir, fn) for fn in os.listdir(input_dir) if '{}_'.format(sample_name) in fn][0]\n",
    "            \n",
    "            slices_dir = os.path.join(sample_dir, 'particles')\n",
    "            slice_paths = sorted([os.path.join(slices_dir, fn) for fn in os.listdir(slices_dir)])\n",
    "            input_data = np.array([io.imread(fp) for fp in slice_paths])\n",
    "            \n",
    "            objects_stats, labeled_data = object_counter(input_data)\n",
    "            output_path = os.path.join(input_dir, sample_dir, 'Analysis')\n",
    "\n",
    "            print 'Data storing - Stats and data saving...'\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "\n",
    "            objects_stats.to_csv(os.path.join(output_path, 'particles_stats_{}.csv'.format(sample_name)))\n",
    "        else:\n",
    "            print '###### Processing of {} skipped'.format(sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_material_relation(sample_name, input_dir, slice_folder, relation_name, rng=None, inverse=False):\n",
    "    print '###### Processing of {}'.format(sample_name) \n",
    "        \n",
    "    sample_dir = [os.path.join(input_dir, fn) for fn in os.listdir(input_dir) if sample_name in fn][0]\n",
    "    slices_dir = os.path.join(sample_dir, slice_folder)\n",
    "        \n",
    "    print 'Data processing - Opening...'\n",
    "            \n",
    "    slice_paths = sorted([os.path.join(slices_dir, fn) for fn in os.listdir(slices_dir)])\n",
    "    if rng is None:\n",
    "        rng = [0, len(slice_paths)-1]\n",
    "    \n",
    "    input_data = np.array([io.imread(fp) for fp in slice_paths[rng[0]:rng[-1]]])\n",
    "   \n",
    "    total_volume = input_data[0].size * len(slice_paths)\n",
    "    material_volume = np.count_nonzero(input_data)\n",
    "    print '{}/{}'.format(total_volume, material_volume)\n",
    "            \n",
    "    output_path = os.path.join(input_dir, sample_dir, 'Analysis')\n",
    "            \n",
    "    print 'Data storing - Stats and data saving...'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    relation = material_volume / float(total_volume)\n",
    "    \n",
    "    if inverse: \n",
    "        relation = 1. - relation\n",
    "        \n",
    "    print 'Relation {}: {}'.format(relation_name, relation)\n",
    "            \n",
    "    np.savetxt(os.path.join(output_path, 'relation_{}_{}.txt'.format(relation_name, sample_name)), [relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_relations(samples, input_dir):\n",
    "    _name_code = {\n",
    "        'fibers': 'porosity',\n",
    "        'particles': 'calcium',\n",
    "        'fibers_no_particles': 'polymer'\n",
    "    }\n",
    "    \n",
    "    for sample_name, configs in samples.items():\n",
    "        components = configs['types']\n",
    "        \n",
    "        for comp in components:\n",
    "            inv = True if comp == 'fibers' else False\n",
    "            rng = configs['rngs'][comp] if ('rngs' in configs) and (comp in configs['rngs']) else None \n",
    "            estimate_material_relation(sample_name, input_dir, comp, _name_code[comp], rng=rng, inverse=inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Processing of sample21\n",
      "Data processing - Opening...\n",
      "903837528/5703631\n",
      "Data storing - Stats and data saving...\n",
      "Relation calcium: 0.00631046047913\n",
      "###### Processing of sample21\n",
      "Data processing - Opening...\n",
      "903837528/111676623\n",
      "Data storing - Stats and data saving...\n",
      "Relation porosity: 0.876441706014\n",
      "###### Processing of sample21\n",
      "Data processing - Opening...\n",
      "903837528/95001515\n",
      "Data storing - Stats and data saving...\n",
      "Relation polymer: 0.105109062256\n",
      "###### Processing of sample22\n",
      "Data processing - Opening...\n",
      "862246800/7104847\n",
      "Data storing - Stats and data saving...\n",
      "Relation calcium: 0.00823992272282\n",
      "###### Processing of sample22\n",
      "Data processing - Opening...\n",
      "862246800/154500966\n",
      "Data storing - Stats and data saving...\n",
      "Relation porosity: 0.820815842981\n",
      "###### Processing of sample22\n",
      "Data processing - Opening...\n",
      "862246800/130817989\n",
      "Data storing - Stats and data saving...\n",
      "Relation polymer: 0.151717569726\n"
     ]
    }
   ],
   "source": [
    "calc_relations(por_samples, './data/samples/porosity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation and diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, dilate_iterations=1, sigma=0.5):\n",
    "    data_8bit = data.astype(np.uint8)\n",
    "    data_8bit = ndi.binary_fill_holes(data_8bit).astype(np.uint8)\n",
    "    skeleton = morphology.skeletonize_3d(data_8bit)\n",
    "    \n",
    "    skeleton_thick = ndi.binary_dilation(skeleton, iterations=dilate_iterations).astype(np.float32)\n",
    "    skeleton_thick = ndi.filters.gaussian_filter(skeleton_thick, sigma)\n",
    "    \n",
    "    return data_8bit, skeleton, skeleton_thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def orientation_3d_tensor_vigra(data, sigma=0.1):\n",
    "    img = vigra.filters.structureTensor(data, 1, 1, sigma_d=sigma)\n",
    "    Axx = img[:,:,:,0]\n",
    "    Axy = img[:,:,:,1]\n",
    "    Axz = img[:,:,:,2]\n",
    "    Ayy = img[:,:,:,3]\n",
    "    Ayz = img[:,:,:,4]\n",
    "    Azz = img[:,:,:,5]\n",
    "        \n",
    "    tensor_vals = np.array([[np.mean(Azz), np.mean(Ayz), np.mean(Axz)],\n",
    "                            [np.mean(Ayz), np.mean(Ayy), np.mean(Axy)],\n",
    "                            [np.mean(Axz), np.mean(Axy), np.mean(Axx)]])\n",
    "    \n",
    "    tensor_vals = tensor_vals[::-1,::-1]\n",
    "    \n",
    "    eps = 1e-8\n",
    "    w, v = np.linalg.eig(tensor_vals)\n",
    "\n",
    "    mv = v[:, np.argmin(w)] # z, y, x\n",
    "    mv[np.abs(mv) < eps] = 0\n",
    "    #print \"MV: \", mv\n",
    "      \n",
    "    G = np.sqrt(mv[2]**2 + mv[1]**2)\n",
    "    #print \"M0 ** 2 + G ** 2 = \", np.sqrt(mv[0] ** 2 + G ** 2)\n",
    "    \n",
    "    #lat =  - np.arctan(G / mv[0]) if mv[0] else np.pi/2.\n",
    "    lat = np.arcsin(np.around(G, decimals=3))\n",
    "    azth = np.arctan(mv[1] / mv[2]) if mv[2] else np.pi/2.\n",
    "#     if azth > 0.:\n",
    "#         azth = np.pi/2. - azth\n",
    "#     else:\n",
    "#         azth = -np.pi/2. - azth\n",
    "    \n",
    "    return lat, azth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_tensor(sample_name, skel, data, window_size, output_dir, sigma=0.025, make_output=True):\n",
    "    output_props = dict()\n",
    "    \n",
    "    Z, Y, X = skel.nonzero()\n",
    "    tens_lat_arr = np.zeros_like(skel, dtype=np.float32)\n",
    "    tens_azth_arr = np.zeros_like(skel, dtype=np.float32)\n",
    "    skel_est = np.zeros_like(skel, dtype=np.int32)\n",
    "    \n",
    "    ws = np.uint32(window_size)\n",
    "    ws2 = ws/2\n",
    "    skel_shape = skel.shape\n",
    "    \n",
    "    output_props['sample_name'] = sample_name\n",
    "    \n",
    "    ts = time.time()\n",
    "    \n",
    "    for idx, pt in enumerate(zip(Z, Y, X)):\n",
    "        lim0 = pt - ws2\n",
    "        lim1 = pt + ws2\n",
    "        \n",
    "        if any(np.array(lim0) < 0) or any(np.array(lim1) > (np.array(skel_shape) - 1)):\n",
    "            skel_est[pt] = -1\n",
    "            continue\n",
    "                \n",
    "        z0, y0, x0 = lim0\n",
    "        z1, y1, x1 = lim1\n",
    "        \n",
    "        area = data[z0:z1, y0:y1, x0:x1]\n",
    "               \n",
    "        \n",
    "        lat, azth = orientation_3d_tensor_vigra(area, sigma)\n",
    "        \n",
    "        tens_lat_arr[pt] = lat\n",
    "        tens_azth_arr[pt] = azth\n",
    "        skel_est[pt] = 255\n",
    "        \n",
    "    te = time.time()\n",
    "    output_props['time'] = te-ts\n",
    "    \n",
    "    print \"Tensor time: %fs\" % (output_props['time'])\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    if make_output:\n",
    "        opath = os.path.join(output_dir, '{}_w{}_orientation_evaluation.npy').format(sample_name, window_size)\n",
    "        output_props['opath'] = opath\n",
    "        \n",
    "        output = {'lat': tens_lat_arr, 'azth': tens_azth_arr, 'skeleton': skel_est, 'indices': skel_est > 0}\n",
    "        output['props'] = output_props\n",
    "        np.save(opath, output)\n",
    "    \n",
    "    return output_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def numpy3d_to_array(np_array, allow_surface_bind=True):\n",
    "    # numpy3d_to_array\n",
    "    # this function was\n",
    "    # taken from pycuda mailing list (striped for C ordering only)\n",
    "\n",
    "    d, h, w = np_array.shape\n",
    "\n",
    "    descr = cuda.ArrayDescriptor3D()\n",
    "    descr.width = w\n",
    "    descr.height = h\n",
    "    descr.depth = d\n",
    "    descr.format = cuda.dtype_to_array_format(np_array.dtype)\n",
    "    descr.num_channels = 1\n",
    "    descr.flags = 0\n",
    "\n",
    "    if allow_surface_bind:\n",
    "        descr.flags = cuda.array3d_flags.SURFACE_LDST\n",
    "\n",
    "    device_array = cuda.Array(descr)\n",
    "\n",
    "    copy = cuda.Memcpy3D()\n",
    "    copy.set_src_host(np_array)\n",
    "    copy.set_dst_array(device_array)\n",
    "    copy.width_in_bytes = copy.src_pitch = np_array.strides[1]\n",
    "    copy.src_height = copy.height = h\n",
    "    copy.depth = d\n",
    "\n",
    "    copy()\n",
    "\n",
    "    return device_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_gpu_diameter(data_skel, data_azth, data_lat, data_rad, n_scan_angl=12):\n",
    "    program = SourceModule(\"\"\"\n",
    "    \n",
    "    texture<float, cudaTextureType3D, cudaReadModeElementType> tex_in_azth;\n",
    "    texture<float, cudaTextureType3D, cudaReadModeElementType> tex_in_lat;\n",
    "    texture<float, cudaTextureType3D, cudaReadModeElementType> tex_in_skel;\n",
    "    texture<float, cudaTextureType3D, cudaReadModeElementType> tex_in_rad;\n",
    "\n",
    "    __global__ void diameter3d(int dW, int dH, int dD,\n",
    "                               const float *scan_angl_arr,\n",
    "                               int n_scan_angl,\n",
    "                               float *radius_arr)\n",
    "    {\n",
    "        unsigned int xidx_ = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "\n",
    "        unsigned int ix = xidx_ % dW;\n",
    "        unsigned int iy = threadIdx.y + blockDim.y * blockIdx.y;\n",
    "        unsigned int iz = xidx_ / dW;\n",
    "\n",
    "        if (ix >= dW || iy >= dH || iz >= dD){\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        float _x = (float)ix + 0.5;\n",
    "        float _y = (float)iy + 0.5;\n",
    "        float _z = (float)iz + 0.5;\n",
    "        \n",
    "        if (tex3D(tex_in_skel, _x, _y, _z) == 0) {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        // -----------------------------------------\n",
    "        // Find the diameter\n",
    "        \n",
    "        float best_azth = tex3D(tex_in_azth, _x, _y, _z);\n",
    "        float best_lat = tex3D(tex_in_lat, _x, _y, _z);\n",
    "        \n",
    "        float best_dz = cos(best_lat);\n",
    "        float dxy = sin(best_lat);\n",
    "        float best_dx = cos(best_azth) * dxy;\n",
    "        float best_dy = -sin(best_azth) * dxy;\n",
    "        \n",
    "        float cy = cosf(best_lat), sy = sinf(best_lat);\n",
    "        float cz = cosf(-best_azth), sz = sinf(-best_azth);\n",
    "        \n",
    "        float scan_vec[3] = {1, 0, 0};\n",
    "        float rot_scan_vec[3] = {scan_vec[0]*cy*cz - scan_vec[1]*sy + scan_vec[2]*cz*sy, \n",
    "                                 scan_vec[0]*sz*cy + scan_vec[1]*cz + scan_vec[2]*sy*sz, \n",
    "                                 -scan_vec[0]*sy + scan_vec[2]*cy};\n",
    "                                \n",
    "        float min_radius = dD;\n",
    "        float max_radius = -1;\n",
    "        int max_iters = 20;\n",
    "                                \n",
    "        for (int scan_angl_idx = 0; scan_angl_idx < n_scan_angl; scan_angl_idx++) {                    \n",
    "            float theta = scan_angl_arr[scan_angl_idx];\n",
    "            \n",
    "            float ct = cosf(theta), st = sinf(theta);\n",
    "            float x = rot_scan_vec[0], y = rot_scan_vec[1], z = rot_scan_vec[2];\n",
    "            float u = best_dx, v = best_dy, w = best_dz;\n",
    "            \n",
    "            float scan_vec_coords[3] = \n",
    "                    {u*(u*x + v*y + w*z)*(1.0f - ct) + x*ct + (-w*y + v*z)*st,\n",
    "                     v*(u*x + v*y + w*z)*(1.0f - ct) + y*ct + (w*x - u*z)*st,\n",
    "                     w*(u*x + v*y + w*z)*(1.0f - ct) + z*ct + (-v*x + u*y)*st};\n",
    "            \n",
    "            float nc[3] = {_x, _y, _z};\n",
    "            float p[3];\n",
    "            \n",
    "            for (int i = 0; i < max_iters; i++) {\n",
    "                nc[0] += scan_vec_coords[0];\n",
    "                nc[1] += scan_vec_coords[1];\n",
    "                nc[2] += scan_vec_coords[2];\n",
    "                \n",
    "                if ((tex3D(tex_in_rad, nc[0], nc[1], nc[2]) == 0) &&\n",
    "                    (nc[0] >= 0 && nc[0] < dW) &&\n",
    "                    (nc[1] >= 0 && nc[1] < dH) &&\n",
    "                    (nc[2] >= 0 && nc[2] < dD)) {\n",
    "                    \n",
    "                    p[0] = nc[0];\n",
    "                    p[1] = nc[1];\n",
    "                    p[2] = nc[2];\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            float cur_radius = norm3df(p[0] - _x, p[1] - _y, p[2] - _z);\n",
    "\n",
    "            if (cur_radius < min_radius) {\n",
    "                min_radius = cur_radius;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        int output_index = ix + iy * dW + iz * (dW * dH);\n",
    "       \n",
    "        radius_arr[output_index] = min_radius;\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # ---- Generate data\n",
    "    data_skel = np.float32(data_skel)\n",
    "    data_azth = np.float32(data_azth)\n",
    "    data_lat = np.float32(data_lat)\n",
    "    data_rad = np.float32(data_rad)\n",
    "    \n",
    "    data_len = reduce(lambda x,y: x * y, data_skel.shape)\n",
    "    \n",
    "    print \"Data generated!\"\n",
    "\n",
    "    # ---- Allocate memory for results\n",
    "    radius_arr = np.zeros(data_len, dtype=np.float32)\n",
    "\n",
    "    # ---- Generate angles\n",
    "    n_scan_angl = np.int32(n_scan_angl)\n",
    "    scan_angl_arr = np.float32(np.linspace(0, 360, num=n_scan_angl, endpoint=False))\n",
    "    scan_angl_arr = np.deg2rad(scan_angl_arr)\n",
    "    \n",
    "    # ---- Set values\n",
    "    dD, dH, dW = np.int32(data_skel.shape)\n",
    "    print \"dW, dH, dD\", dW, dH, dD\n",
    "\n",
    "    # ---- Allocate gpu memory\n",
    "    gpu_scan_angl_arr = cuda.mem_alloc(scan_angl_arr.nbytes)\n",
    "    gpu_radius_arr = cuda.mem_alloc(radius_arr.nbytes)\n",
    "\n",
    "    # ---- Copy host to device\n",
    "    cuda.memcpy_htod(gpu_scan_angl_arr, scan_angl_arr)\n",
    "    cuda.memcpy_htod(gpu_radius_arr, radius_arr)\n",
    "\n",
    "    # ---- Get kernel\n",
    "    diameter3d = program.get_function(\"diameter3d\")\n",
    "\n",
    "    # ---- Bind texture\n",
    "    tex_in_azth = program.get_texref('tex_in_azth')\n",
    "    cuda_array_in_azth = numpy3d_to_array(data_azth)\n",
    "    tex_in_azth.set_array(cuda_array_in_azth)\n",
    "    \n",
    "    tex_in_lat = program.get_texref('tex_in_lat')\n",
    "    cuda_array_in_lat = numpy3d_to_array(data_lat)\n",
    "    tex_in_lat.set_array(cuda_array_in_lat)\n",
    "    \n",
    "    tex_in_skel = program.get_texref('tex_in_skel')\n",
    "    cuda_array_in_skel = numpy3d_to_array(data_skel)\n",
    "    tex_in_skel.set_array(cuda_array_in_skel)\n",
    "\n",
    "    tex_in_rad = program.get_texref('tex_in_rad')\n",
    "    cuda_array_in_rad = numpy3d_to_array(data_rad)\n",
    "    tex_in_rad.set_array(cuda_array_in_rad)\n",
    "    \n",
    "    # Compute dimensions and run the kernel\n",
    "    # NOTE: No support for 3D grid in pycuda\n",
    "    bdim = (16, 32, 1)\n",
    "    dx, mx = divmod(dD * dH, bdim[0])\n",
    "    dy, my = divmod(dW, bdim[1])\n",
    "    gdim = ((dx + (mx > 0)) * bdim[0],\n",
    "            (dy + (my > 0)) * bdim[1])\n",
    "\n",
    "    start = cuda.Event()\n",
    "    end = cuda.Event()\n",
    "\n",
    "    start.record()  # start timing\n",
    "    diameter3d(dW, dH, dD,\n",
    "               gpu_scan_angl_arr,\n",
    "               n_scan_angl,\n",
    "               gpu_radius_arr,\n",
    "               block=bdim,\n",
    "               grid=gdim)\n",
    "\n",
    "    end.record()  # end timing\n",
    "    end.synchronize()\n",
    "    secs = start.time_till(end)*1e-3\n",
    "\n",
    "    # Copy result from device to host memory\n",
    "    cuda.memcpy_dtoh(radius_arr, gpu_radius_arr)\n",
    "    \n",
    "    gpu_radius_arr.free()\n",
    "    gpu_scan_angl_arr.free()\n",
    "    \n",
    "    cuda_array_in_azth.free()\n",
    "    cuda_array_in_lat.free()\n",
    "    cuda_array_in_skel.free()\n",
    "    cuda_array_in_rad.free()\n",
    "\n",
    "    return radius_arr, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantify_diameter(sample_name, data_skel, data_azth, data_lat, data_rad, n_scan_angl, output_dir, do_reshape=True):\n",
    "    data_skel[data_skel < 0] = 0\n",
    "    dmtr, elapsed_time = run_gpu_diameter(data_skel, data_azth, data_lat, data_rad, n_scan_angl=n_scan_angl)\n",
    "    \n",
    "    print 'Diameter time: {}s'.format(elapsed_time)\n",
    "\n",
    "    if do_reshape:\n",
    "        dmtr = np.reshape(dmtr, data_skel.shape, order='C')\n",
    "        \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    opath = os.path.join(output_dir, '{}_nsa{}_diameter_evaluation.npy').format(sample_name, n_scan_angl)\n",
    "    output_props = {'opath': opath, 'time': elapsed_time, 'sample_name': sample_name}  \n",
    "    output = {'diameter': dmtr, 'skeleton': data_skel, 'indices': data_skel > 0, 'props': output_props}\n",
    "    np.save(opath, output)\n",
    "    \n",
    "    return output_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_fiber_properties(samples, input_dir, window_size=32, num_scanning_angles=30, type='fibers'):\n",
    "    for sample_name, configs in samples.items():\n",
    "        print '###### Orientation and diameter processing of {}'.format(sample_name) \n",
    "        \n",
    "        bin_data = open_data(os.path.join(input_dir, sample_name, type))\n",
    "        data_8bit, skel, skel_thick = prepare_data(bin_data)\n",
    "        analysis_odir = os.path.join(input_dir, sample_name, 'Analysis')\n",
    "        \n",
    "        orient_props = estimate_tensor(sample_name, skel, skel_thick, \n",
    "                                       window_size, analysis_odir)\n",
    "        orient_data = np.load(orient_props['opath']).item()\n",
    "        \n",
    "        diamtr_props = quantify_diameter(sample_name,\n",
    "                                         orient_data['skeleton'], \n",
    "                                         orient_data['azth'],\n",
    "                                         orient_data['lat'],\n",
    "                                         data_8bit,\n",
    "                                         num_scanning_angles,\n",
    "                                         analysis_odir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Orientation and diameter processing of sample21\n",
      "Tensor time: 37812.243759s\n",
      "Data generated!\n",
      "dW, dH, dD 1200 550 370\n",
      "Diameter time: 4.19169433594s\n",
      "###### Orientation and diameter processing of sample22\n"
     ]
    }
   ],
   "source": [
    "estimate_fiber_properties(seg_samples, './data/samples/fiber_analysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
