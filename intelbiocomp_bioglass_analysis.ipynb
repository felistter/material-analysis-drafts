{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import ast\n",
    "import itertools\n",
    "from StringIO import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, filters, measure, feature, exposure, color, morphology, draw\n",
    "from skimage.feature import register_translation\n",
    "from skimage import transform as tf\n",
    "import fabio as fb\n",
    "import mahotas as mh\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "_MEASUREMENTS = {\n",
    "    'Label': 'label',\n",
    "    'Area': 'area',\n",
    "    'Perimeter': 'perimeter'\n",
    "}\n",
    "\n",
    "_MEASUREMENTS_VALS = _MEASUREMENTS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#https://github.com/zenr/ippy\n",
    "\n",
    "\"\"\"\n",
    "Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "Usage: $ python max_entropy.py <gray scale image>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def max_entropy(data):\n",
    "    \"\"\"\n",
    "    Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "    Kapur J.N., Sahoo P.K., and Wong A.K.C. (1985) \"A New Method for Gray-Level Picture Thresholding Using the Entropy\n",
    "    of the Histogram\", Graphical Models and Image Processing, 29(3): 273-285\n",
    "    M. Emre Celebi\n",
    "    06.15.2007\n",
    "    Ported to ImageJ plugin by G.Landini from E Celebi's fourier_0.8 routines\n",
    "    2016-04-28: Adapted for Python 2.7 by Robert Metchev from Java source of MaxEntropy() in the Autothresholder plugin\n",
    "    http://rsb.info.nih.gov/ij/plugins/download/AutoThresholder.java\n",
    "    :param data: Sequence representing the histogram of the image\n",
    "    :return threshold: Resulting maximum entropy threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate CDF (cumulative density function)\n",
    "    cdf = data.astype(np.float).cumsum()\n",
    "\n",
    "    # find histogram's nonzero area\n",
    "    valid_idx = np.nonzero(data)[0]\n",
    "    first_bin = valid_idx[0]\n",
    "    last_bin = valid_idx[-1]\n",
    "\n",
    "    # initialize search for maximum\n",
    "    max_ent, threshold = 0, 0\n",
    "\n",
    "    for it in range(first_bin, last_bin + 1):\n",
    "        # Background (dark)\n",
    "        hist_range = data[:it + 1]\n",
    "        hist_range = hist_range[hist_range != 0] / cdf[it]  # normalize within selected range & remove all 0 elements\n",
    "        tot_ent = -np.sum(hist_range * np.log(hist_range))  # background entropy\n",
    "\n",
    "        # Foreground/Object (bright)\n",
    "        hist_range = data[it + 1:]\n",
    "        # normalize within selected range & remove all 0 elements\n",
    "        hist_range = hist_range[hist_range != 0] / (cdf[last_bin] - cdf[it])\n",
    "        tot_ent -= np.sum(hist_range * np.log(hist_range))  # accumulate object entropy\n",
    "\n",
    "        # find max\n",
    "        if tot_ent > max_ent:\n",
    "            max_ent, threshold = tot_ent, it\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tryeval(val):\n",
    "    try:\n",
    "        val = ast.literal_eval(val)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path):\n",
    "    filelist = [f for f in os.listdir(dir_path) if f.endswith('.tif')]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(dir_path, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_as_raw(data, sample_name, output_dir, prefix=None):\n",
    "    bits = -1\n",
    "    if data.dtype == np.int32 or data.dtype == np.float32:\n",
    "        bits = 32\n",
    "    elif data.dtype == np.uint8 or data.dtype == np.bool:\n",
    "        bits = 8\n",
    "\n",
    "    size = data.shape[::-1]\n",
    "    output_filename = '{0}_{1}bit_{2}x{3}x{4}.raw'.format(sample_name, bits, *size) if prefix is None \\\n",
    "                        else '{0}_{1}_{2}bit_{3}x{4}x{5}.raw'.format(sample_name, prefix, bits, *size)\n",
    "    data.tofile(os.path.join(output_dir, output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_gdocs(url):\n",
    "    import requests\n",
    "    r = requests.get(url)\n",
    "    data = r.content\n",
    "    df = pd.read_csv(StringIO(data))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_samples_configs(df):\n",
    "    columns = {'Offsets (start, end)': 'offset', \n",
    "               'Slice bounding box(x, y, w, h)': 'bbox', \n",
    "               'Analysis': 'analysis_type', \n",
    "               'Type': 'structure', \n",
    "               'Radius': 'radius', \n",
    "               'Slice folder': 'slice_folder',\n",
    "               'Particles density': 'density', \n",
    "               'Center': 'center', \n",
    "               'Priority': 'priority'}\n",
    "\n",
    "    cols = df.columns[df.columns.isin(columns.keys())]\n",
    "    \n",
    "    out = dict.fromkeys(df['Name'].values, None)\n",
    "    for name in df['Name'].values:\n",
    "        out[name] = df[df['Name'] == name][cols].to_dict(orient='records')\n",
    "        out[name] = {columns[k]: tryeval(v) for k, v in out[name][0].items()}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def object_counter(binary_data):\n",
    "    labeled_stack, num_labels = ndi.measurements.label(binary_data)\n",
    "    objects_stats = pd.DataFrame(columns=_MEASUREMENTS_VALS)\n",
    "\n",
    "    labeled_stack_expanded = labeled_stack[np.newaxis,:,:] \\\n",
    "                                if len(labeled_stack.shape) == 2 else labeled_stack\n",
    "\n",
    "    for labeled_slice in labeled_stack_expanded:\n",
    "        for region in measure.regionprops(labeled_slice):\n",
    "            objects_stats = objects_stats.append({_measure: region[_measure] \\\n",
    "                                            for _measure in _MEASUREMENTS_VALS}, \\\n",
    "                                                ignore_index=True)\n",
    "\n",
    "    objects_stats = objects_stats.groupby('label', as_index=False).sum()\n",
    "\n",
    "    return objects_stats, labeled_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_particles(data, max_area=2000):\n",
    "    data = data.copy()\n",
    "    stats, labels = object_counter(data)\n",
    "    stats = stats[stats['area'] > max_area]\n",
    "    for index, row in stats.iterrows():\n",
    "        labels[labels == row['label']] = 0\n",
    "        \n",
    "    labels[np.nonzero(labels)] = 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eliminate_structure_holes(mask, max_area=2000):\n",
    "    data_mask = ndi.morphology.binary_closing(mask, structure=morphology.disk(2), iterations=3)\n",
    "    \n",
    "    data_mask_filled = data_mask.copy()\n",
    "    data_mask_filled = ndi.morphology.binary_fill_holes(data_mask_filled)\n",
    "\n",
    "    mask_diff = data_mask_filled - data_mask\n",
    "\n",
    "    data_labeled = filter_particles(mask_diff, max_area=max_area)\n",
    "    data_labeled = np.logical_or(data_labeled, data_mask).astype(np.uint8)\n",
    "    \n",
    "    return data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask_with_circle_ROI(data, radius):\n",
    "    masked_data = mask_data_with_circle(data, radius)\n",
    "    \n",
    "    p2 = np.percentile(masked_data, 0.01)\n",
    "    p98 = np.percentile(masked_data, 99.99)\n",
    "\n",
    "    data_8bit = exposure.rescale_intensity(masked_data, \\\n",
    "                                           in_range=(p2, p98), \\\n",
    "                                           out_range=np.uint8).astype(np.uint8)\n",
    "    data_8bit = filters.median(data_8bit, \\\n",
    "                               selem=morphology.disk(5))\n",
    "\n",
    "    th = mh.otsu(data_8bit, ignore_zeros=True)\n",
    "\n",
    "    mask = data_8bit >= th\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask = ndi.morphology.binary_fill_holes(mask)\n",
    "    \n",
    "    return data_8bit, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_data_with_circle(data, radius, center=None):\n",
    "    data_shape = data.shape\n",
    "    \n",
    "    if center is None:\n",
    "        center = [v/2. for v in data_shape]\n",
    "        \n",
    "    rr,cc = draw.ellipse(center[1], center[0], radius, radius, shape=data_shape)\n",
    "    circ_mask_roi = np.zeros(data_shape, dtype=np.uint8)\n",
    "    circ_mask_roi[rr,cc] = 1\n",
    "    masked_data = data * circ_mask_roi\n",
    "    clipped_data = masked_data[(center[1]-radius):(center[1]+radius), \\\n",
    "                               (center[0]-radius):(center[0]+radius)]\n",
    "    \n",
    "    return masked_data, clipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def segment_sample_with_circle_ROI(samples_info,\n",
    "                                   input_dir_tmpl='./data/intelbiocomp_bioglass_raw_data/recon/{sample_name}/{slice_folder}',\n",
    "                                   output_dir_tmpl='./data/intelbiocomp_bioglass_results', \n",
    "                                   median_filter_rad=2):\n",
    "    \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        start = timer()\n",
    "        \n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        radius = configs['radius']\n",
    "        atype = configs['analysis_type'].strip()\n",
    "        slice_folder = configs['slice_folder']\n",
    "        srgn = configs['offset']\n",
    "        dsty = configs['density']\n",
    "        center = configs['center']\n",
    "        \n",
    "        components = atype.split('/')\n",
    "        \n",
    "        def _create_dir(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            else:\n",
    "                shutil.rmtree(path)\n",
    "                os.makedirs(path)\n",
    "\n",
    "            return path\n",
    "        \n",
    "        crop_output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, 'cropped'))\n",
    "        clipped_output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, 'clipped_roi'))\n",
    "        \n",
    "        input_dir = input_dir_tmpl.format(sample_name=sample_name, slice_folder=slice_folder)\n",
    "        \n",
    "        print input_dir\n",
    "        print crop_output_dir\n",
    "        print clipped_output_dir\n",
    "        \n",
    "        filenames = os.listdir(input_dir)\n",
    "        filenames = [fn for fn in filenames if fn.endswith('.tif')]\n",
    "        files = [os.path.join(input_dir, f) for f in filenames]\n",
    "        \n",
    "        for fpath in itertools.islice(sorted(files), srgn[0], len(files) + srgn[1]):\n",
    "            data = fb.open(fpath).data\n",
    "            filename = os.path.basename(fpath)\n",
    "            \n",
    "            cropped_data, clipped_data = mask_data_with_circle(data, radius, center=center)\n",
    "            \n",
    "            io.imsave(os.path.join(crop_output_dir, filename), cropped_data)\n",
    "            io.imsave(os.path.join(clipped_output_dir, filename), clipped_data)\n",
    "\n",
    "        for comp in components:\n",
    "            output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, comp.lower() + '_masks'))\n",
    "\n",
    "            filenames = os.listdir(crop_output_dir)\n",
    "            filenames = [fn for fn in filenames if fn.endswith('.tif')]\n",
    "            files = [os.path.join(crop_output_dir, f) for f in filenames]\n",
    "\n",
    "            for i, fpath in enumerate(sorted(files)):\n",
    "                cropped_data = fb.open(fpath).data\n",
    "                filename = os.path.basename(fpath)\n",
    "\n",
    "                if i % 250 == 0 or i == (len(files) - 1):\n",
    "                    print '%d/%d' % (i, len(files) - 1)\n",
    "                    \n",
    "                p2 = np.percentile(cropped_data, 0.01)\n",
    "                p98 = np.percentile(cropped_data, 99.99)\n",
    "        \n",
    "                cropped_data = exposure.rescale_intensity(cropped_data, in_range=(p2, p98))\n",
    "                data_8bit = exposure.rescale_intensity(cropped_data, in_range='image', out_range=np.uint8).astype(np.uint8)\n",
    "                data_8bit = filters.median(data_8bit, selem=morphology.disk(median_filter_rad))\n",
    "                \n",
    "                th = max_entropy(np.histogram(data_8bit, bins=256, range=(0, 256))[0]) if dsty == 'sparse' else \\\n",
    "                        mh.otsu(data_8bit, ignore_zeros=True)\n",
    "                    \n",
    "                mask = data_8bit >= th \n",
    "                mask = mask.astype(np.uint8)\n",
    "                \n",
    "                io.imsave(os.path.join(output_dir, filename), mask.astype(np.uint8))\n",
    "            \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_features(samples_info, \\\n",
    "                       input_dir_tmpl='./data/intelbiocomp_bioglass_results', \\\n",
    "                       roi_box=((-175,175),(-175,175),(-175,175))):\n",
    "    \n",
    "    roi_side = np.sum(np.abs(roi_box[0]))\n",
    "    \n",
    "    def _create_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        start = timer()\n",
    "        \n",
    "        center = configs['center']\n",
    "        \n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        atype = configs['analysis_type'].strip()\n",
    "        components = atype.lower().split('/')\n",
    "        \n",
    "        if 'polymer' not in atype.lower() and \\\n",
    "           'particle' not in atype.lower():\n",
    "                raise ValueError('The sample component(s) are not specified.')\n",
    "        \n",
    "        for comp in components: \n",
    "            input_dir = os.path.join(input_dir_tmpl, sample_name, comp.lower() + '_masks')\n",
    "            \n",
    "            filenames = os.listdir(input_dir)\n",
    "            files = [os.path.join(input_dir, f) for f in filenames]\n",
    "            \n",
    "            try:\n",
    "                slice_shape = io.imread(files[0]).shape\n",
    "            except:\n",
    "                slice_shape = fb.open(files[0]).data.shape\n",
    "            \n",
    "            print 'Porosity calculation...'\n",
    "            output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'porosity_stats_{}'.format(roi_side)))\n",
    "                \n",
    "            if not len(files):\n",
    "                raise ValueError('No mask files.')\n",
    "\n",
    "            slice_area = slice_shape[0] * slice_shape[1]\n",
    "                \n",
    "            try:\n",
    "                porosity_comps = [np.count_nonzero(io.imread(fpath)) / float(slice_area) \\\n",
    "                                    for fpath in files]\n",
    "            except:\n",
    "                porosity_comps = [np.count_nonzero(fb.open(fpath).data) / float(slice_area) \\\n",
    "                                    for fpath in files]\n",
    "\n",
    "            pd.DataFrame({'Porosity': [sum(porosity_comps)]}, \\\n",
    "                            index=[sample_name]).to_csv(os.path.join(output_dir, 'porosity.csv'))\n",
    "                \n",
    "            print 'Particles counting...'\n",
    "            output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'particles_stats_{}'.format(roi_side)))\n",
    "            \n",
    "            depth, height, width  = len(files), slice_shape[0], slice_shape[1]\n",
    "            cd, ch, cw = depth/2, height/2, width/2\n",
    "                \n",
    "            if roi_box is not None:\n",
    "                data_mask_vol = np.zeros(tuple([np.sum(np.abs(v)) for v in roi_box]), dtype=np.uint8)\n",
    "            else:\n",
    "                data_mask_vol = np.zeros((depth, height, width), dtype=np.uint8)\n",
    "            \n",
    "            path_data = itertools.islice(sorted(files), cd+roi_box[0][0], cd+roi_box[0][1]) if roi_box is not None \\\n",
    "                        else sorted(files)\n",
    "            \n",
    "            for i, fpath in enumerate(path_data):\n",
    "                try:\n",
    "                    data_slice = io.imread(fpath)\n",
    "                except:\n",
    "                    data_slice = fb.open(fpath).data\n",
    "                    \n",
    "                data_slice = data_slice[slice(center[1]+roi_box[1][0], center[1]+roi_box[1][1], 1), \\\n",
    "                                        slice(center[0]+roi_box[2][0], center[0]+roi_box[2][1], 1)]\n",
    "                    \n",
    "                data_mask_vol[i] = data_slice\n",
    "                \n",
    "            print data_mask_vol.shape, data_mask_vol.dtype\n",
    "        \n",
    "            particle_stats, labeled_data = object_counter(data_mask_vol)\n",
    "            particle_stats.to_csv(os.path.join(output_dir, 'particles.csv'))\n",
    "            \n",
    "            write_as_raw(labeled_data.astype(np.uint8), sample_name, output_dir)\n",
    "        \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_sample(samples_configs, \\\n",
    "                   path_template='./data/intelbiocomp_bioglass_raw_data/recon/biomaterials/materials/{0}/tomo1/{1}', \\\n",
    "                   in_folder='slices_phase_particles', \\\n",
    "                   max_area=2500):\n",
    "    for sample_name, configs in samples_configs.iteritems():\n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        bbox = configs['bbox']\n",
    "        atype = configs['analysis_type'].strip()\n",
    "        struct_type = configs['structure']\n",
    "        \n",
    "        components = atype.split('/')\n",
    "        \n",
    "        data_output_dir = path_template.format(sample_name, 'cropped_data')\n",
    "        if not os.path.exists(data_output_dir):\n",
    "            os.makedirs(data_output_dir)\n",
    "        else:\n",
    "            shutil.rmtree(data_output_dir)\n",
    "            os.makedirs(data_output_dir)\n",
    "            \n",
    "        input_dir = path_template.format(sample_name, in_folder)\n",
    "        \n",
    "        for comp in components:\n",
    "            output_dir = path_template.format(sample_name, comp.lower() + '_masks')\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            else:\n",
    "                shutil.rmtree(output_dir)\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            filenames = os.listdir(input_dir)\n",
    "            files = [os.path.join(input_dir, f) for f in filenames]\n",
    "\n",
    "            working_files = files[configs['offset'][0]: configs['offset'][1]]\n",
    "\n",
    "            for i, fpath in enumerate(working_files):\n",
    "                data = io.imread(fpath)\n",
    "\n",
    "                filename = os.path.basename(fpath)\n",
    "\n",
    "                if i % 200 == 0 or i == (len(working_files) - 1):\n",
    "                    print '%d/%d' % (i, len(working_files) - 1)\n",
    "\n",
    "                data = data[bbox[1]:(bbox[1] + bbox[3] + 1), bbox[0]:(bbox[1] + bbox[2] + 1)]\n",
    "                p2 = np.percentile(data, 0.01)\n",
    "                p98 = np.percentile(data, 99.99)\n",
    "                data8bit = exposure.rescale_intensity(data, in_range=(p2, p98), out_range=np.uint8).astype(np.uint8)\n",
    "                hist = np.histogram(data8bit, bins=256, range=(0, 256))[0]\n",
    "            \n",
    "                if 'particle' in comp.lower():\n",
    "                    th = max_entropy(hist)\n",
    "                elif 'polymer' in comp.lower():\n",
    "                    th = filters.threshold_otsu(data8bit, nbins=256)\n",
    "                else:\n",
    "                    raise ValueError('Unknown sample type.')\n",
    "\n",
    "                mask = data8bit <= th\n",
    "                mask = mask.astype(np.uint8)\n",
    "                \n",
    "                if 'polymer' in comp.lower() and struct_type == 'Thick':\n",
    "                    mask = eliminate_structure_holes(mask, max_area=max_area)\n",
    "\n",
    "                io.imsave(os.path.join(output_dir, filename), mask)\n",
    "                \n",
    "                cropped_data_path = os.path.join(data_output_dir, filename)\n",
    "                \n",
    "                if not os.path.isfile(cropped_data_path):\n",
    "                    io.imsave(cropped_data_path, data8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_otsu(data):\n",
    "    hist = np.histogram(data.ravel(), 256)[0]\n",
    "    size = float(data.size)\n",
    "    \n",
    "    th1, th2 = 0, 0\n",
    "    mt, max_var = 0, 0\n",
    "    \n",
    "    for k in xrange(256):\n",
    "        mt += k * hist[k] / size\n",
    "        \n",
    "    w0k, m0k = 0, 0\n",
    "    \n",
    "    for t1 in xrange(256):\n",
    "        w0k += hist[t1] / size\n",
    "        m0k += t1 * hist[t1] / size\n",
    "        m0 = m0k / w0k\n",
    "        \n",
    "        w1k, m1k = 0, 0\n",
    "        \n",
    "        for t2 in xrange(1, 256):\n",
    "            w1k += hist[t2] / size\n",
    "            m1k += t2 * hist[t2] / size\n",
    "            m1 = m1k / w1k\n",
    "            \n",
    "            w2k = 1. - (w0k + w1k)\n",
    "            m2k = mt - (m0k + m1k)\n",
    "            \n",
    "            if w2k <= 0:\n",
    "                break\n",
    "                \n",
    "            m2 = m2k / w2k\n",
    "            \n",
    "            curr_var = w0k * (m0 - mt) * (m0 - mt) + w1k * (m1 - mt) * (m1 - mt) + w2k * (m2 - mt) * (m2 - mt)\n",
    "            \n",
    "            if max_var < curr_var:\n",
    "                max_var = curr_var\n",
    "                th1, th2 = t1, t2\n",
    "    \n",
    "    print max_var\n",
    "    return th1, th2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
