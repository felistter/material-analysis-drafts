{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import ast\n",
    "import itertools\n",
    "from StringIO import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, filters, measure, feature, exposure, color, morphology, draw\n",
    "from skimage.feature import register_translation\n",
    "from skimage import transform as tf\n",
    "import fabio as fb\n",
    "import mahotas as mh\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "_MEASUREMENTS = {\n",
    "    'Label': 'label',\n",
    "    'Area': 'area',\n",
    "    'Perimeter': 'perimeter'\n",
    "}\n",
    "\n",
    "_MEASUREMENTS_VALS = _MEASUREMENTS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/zenr/ippy\n",
    "\n",
    "\"\"\"\n",
    "Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "Usage: $ python max_entropy.py <gray scale image>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def max_entropy(data):\n",
    "    \"\"\"\n",
    "    Implements Kapur-Sahoo-Wong (Maximum Entropy) thresholding method\n",
    "    Kapur J.N., Sahoo P.K., and Wong A.K.C. (1985) \"A New Method for Gray-Level Picture Thresholding Using the Entropy\n",
    "    of the Histogram\", Graphical Models and Image Processing, 29(3): 273-285\n",
    "    M. Emre Celebi\n",
    "    06.15.2007\n",
    "    Ported to ImageJ plugin by G.Landini from E Celebi's fourier_0.8 routines\n",
    "    2016-04-28: Adapted for Python 2.7 by Robert Metchev from Java source of MaxEntropy() in the Autothresholder plugin\n",
    "    http://rsb.info.nih.gov/ij/plugins/download/AutoThresholder.java\n",
    "    :param data: Sequence representing the histogram of the image\n",
    "    :return threshold: Resulting maximum entropy threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate CDF (cumulative density function)\n",
    "    cdf = data.astype(np.float).cumsum()\n",
    "\n",
    "    # find histogram's nonzero area\n",
    "    valid_idx = np.nonzero(data)[0]\n",
    "    first_bin = valid_idx[0]\n",
    "    last_bin = valid_idx[-1]\n",
    "\n",
    "    # initialize search for maximum\n",
    "    max_ent, threshold = 0, 0\n",
    "\n",
    "    for it in range(first_bin, last_bin + 1):\n",
    "        # Background (dark)\n",
    "        hist_range = data[:it + 1]\n",
    "        hist_range = hist_range[hist_range != 0] / cdf[it]  # normalize within selected range & remove all 0 elements\n",
    "        tot_ent = -np.sum(hist_range * np.log(hist_range))  # background entropy\n",
    "\n",
    "        # Foreground/Object (bright)\n",
    "        hist_range = data[it + 1:]\n",
    "        # normalize within selected range & remove all 0 elements\n",
    "        hist_range = hist_range[hist_range != 0] / (cdf[last_bin] - cdf[it])\n",
    "        tot_ent -= np.sum(hist_range * np.log(hist_range))  # accumulate object entropy\n",
    "\n",
    "        # find max\n",
    "        if tot_ent > max_ent:\n",
    "            max_ent, threshold = tot_ent, it\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tryeval(val):\n",
    "    try:\n",
    "        val = ast.literal_eval(val)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path):\n",
    "    filelist = [f for f in os.listdir(dir_path) if f.endswith('.tif')]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(dir_path, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_as_raw(data, sample_name, output_dir, prefix=None):\n",
    "    bits = -1\n",
    "    if data.dtype == np.int32 or data.dtype == np.float32:\n",
    "        bits = 32\n",
    "    elif data.dtype == np.uint8 or data.dtype == np.bool:\n",
    "        bits = 8\n",
    "\n",
    "    size = data.shape[::-1]\n",
    "    output_filename = '{0}_{1}bit_{2}x{3}x{4}.raw'.format(sample_name, bits, *size) if prefix is None \\\n",
    "                        else '{0}_{1}_{2}bit_{3}x{4}x{5}.raw'.format(sample_name, prefix, bits, *size)\n",
    "    data.tofile(os.path.join(output_dir, output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_gdocs(url):\n",
    "    import requests\n",
    "    r = requests.get(url, verify=False)\n",
    "    data = r.content\n",
    "    df = pd.read_csv(StringIO(data))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_samples_configs(df):\n",
    "    columns = {'Offsets (start, end)': 'offset', \n",
    "               'Slice bounding box(x, y, w, h)': 'bbox', \n",
    "               'Analysis': 'analysis_type', \n",
    "               'Type': 'structure', \n",
    "               'Radius': 'radius', \n",
    "               'Slice folder': 'slice_folder',\n",
    "               'Particles density': 'density', \n",
    "               'Center': 'center', \n",
    "               'Priority': 'priority'}\n",
    "\n",
    "    cols = df.columns[df.columns.isin(columns.keys())]\n",
    "    \n",
    "    out = dict.fromkeys(df['Name'].values, None)\n",
    "    for name in df['Name'].values:\n",
    "        out[name] = df[df['Name'] == name][cols].to_dict(orient='records')\n",
    "        out[name] = {columns[k]: tryeval(v) for k, v in out[name][0].items()}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def object_counter(binary_data):\n",
    "    labeled_stack, num_labels = ndi.measurements.label(binary_data)\n",
    "    objects_stats = pd.DataFrame(columns=_MEASUREMENTS_VALS)\n",
    "\n",
    "    labeled_stack_expanded = labeled_stack[np.newaxis,:,:] \\\n",
    "                                if len(labeled_stack.shape) == 2 else labeled_stack\n",
    "\n",
    "    for labeled_slice in labeled_stack_expanded:\n",
    "        for region in measure.regionprops(labeled_slice):\n",
    "            objects_stats = objects_stats.append({_measure: region[_measure] \\\n",
    "                                            for _measure in _MEASUREMENTS_VALS}, \\\n",
    "                                                ignore_index=True)\n",
    "\n",
    "    objects_stats = objects_stats.groupby('label', as_index=False).sum()\n",
    "\n",
    "    return objects_stats, labeled_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_particles(data, max_area=2000):\n",
    "    data = data.copy()\n",
    "    stats, labels = object_counter(data)\n",
    "    stats = stats[stats['area'] > max_area]\n",
    "    for index, row in stats.iterrows():\n",
    "        labels[labels == row['label']] = 0\n",
    "        \n",
    "    labels[np.nonzero(labels)] = 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eliminate_structure_holes(mask, max_area=2000):\n",
    "    data_mask = ndi.morphology.binary_closing(mask, structure=morphology.disk(2), iterations=3)\n",
    "    \n",
    "    data_mask_filled = data_mask.copy()\n",
    "    data_mask_filled = ndi.morphology.binary_fill_holes(data_mask_filled)\n",
    "\n",
    "    mask_diff = data_mask_filled - data_mask\n",
    "\n",
    "    data_labeled = filter_particles(mask_diff, max_area=max_area)\n",
    "    data_labeled = np.logical_or(data_labeled, data_mask).astype(np.uint8)\n",
    "    \n",
    "    return data_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask_with_circle_ROI(data, radius):\n",
    "    masked_data = mask_data_with_circle(data, radius)\n",
    "    \n",
    "    p2 = np.percentile(masked_data, 0.01)\n",
    "    p98 = np.percentile(masked_data, 99.99)\n",
    "\n",
    "    data_8bit = exposure.rescale_intensity(masked_data, \\\n",
    "                                           in_range=(p2, p98), \\\n",
    "                                           out_range=np.uint8).astype(np.uint8)\n",
    "    data_8bit = filters.median(data_8bit, \\\n",
    "                               selem=morphology.disk(5))\n",
    "\n",
    "    th = mh.otsu(data_8bit, ignore_zeros=True)\n",
    "\n",
    "    mask = data_8bit >= th\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask = ndi.morphology.binary_fill_holes(mask)\n",
    "    \n",
    "    return data_8bit, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_data_with_circle(data, radius, center=None):\n",
    "    data_shape = data.shape\n",
    "    \n",
    "    if center is None:\n",
    "        center = [v/2. for v in data_shape]\n",
    "        \n",
    "    rr,cc = draw.ellipse(center[1], center[0], radius, radius, shape=data_shape)\n",
    "    circ_mask_roi = np.zeros(data_shape, dtype=np.uint8)\n",
    "    circ_mask_roi[rr,cc] = 1\n",
    "    masked_data = data * circ_mask_roi\n",
    "    clipped_data = masked_data[(center[1]-radius):(center[1]+radius), \\\n",
    "                               (center[0]-radius):(center[0]+radius)]\n",
    "    \n",
    "    return masked_data, clipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_sample_with_circle_ROI(samples_info,\n",
    "                                   input_dir_tmpl='./data/pcl_raw_data/{sample_name}/{slice_folder}',\n",
    "                                   output_dir_tmpl='./data/pcl_results', \n",
    "                                   median_filter_rad=1):\n",
    "    \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        start = timer()\n",
    "        \n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        radius = configs['radius']\n",
    "        atype = configs['analysis_type'].strip()\n",
    "        slice_folder = configs['slice_folder']\n",
    "        srgn = configs['offset']\n",
    "        dsty = configs['density']\n",
    "        center = configs['center']\n",
    "        \n",
    "        components = atype.split('/')\n",
    "        \n",
    "        def _create_dir(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            else:\n",
    "                shutil.rmtree(path)\n",
    "                os.makedirs(path)\n",
    "\n",
    "            return path\n",
    "        \n",
    "        crop_output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, 'cropped'))\n",
    "        clipped_output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, 'clipped_roi'))\n",
    "        \n",
    "        input_dir = input_dir_tmpl.format(sample_name=sample_name, slice_folder=slice_folder)\n",
    "        \n",
    "        print input_dir\n",
    "        print crop_output_dir\n",
    "        print clipped_output_dir\n",
    "        \n",
    "        filenames = os.listdir(input_dir)\n",
    "        filenames = [fn for fn in filenames if fn.endswith('.tif')]\n",
    "        files = [os.path.join(input_dir, f) for f in filenames]\n",
    "        \n",
    "        for fpath in itertools.islice(sorted(files), srgn[0], len(files) + srgn[1]):\n",
    "            data = fb.open(fpath).data\n",
    "            filename = os.path.basename(fpath)\n",
    "            \n",
    "            #cropped_data, clipped_data = mask_data_with_circle(data, radius, center=center)\n",
    "            \n",
    "            cropped_data = data\n",
    "            \n",
    "            io.imsave(os.path.join(crop_output_dir, filename), cropped_data)\n",
    "            #io.imsave(os.path.join(clipped_output_dir, filename), clipped_data)\n",
    "\n",
    "        for comp in components:\n",
    "            output_dir = _create_dir(os.path.join(output_dir_tmpl, sample_name, comp.lower() + '_masks'))\n",
    "\n",
    "            filenames = os.listdir(crop_output_dir)\n",
    "            filenames = [fn for fn in filenames if fn.endswith('.tif')]\n",
    "            files = [os.path.join(crop_output_dir, f) for f in filenames]\n",
    "\n",
    "            for i, fpath in enumerate(sorted(files)):\n",
    "                cropped_data = fb.open(fpath).data\n",
    "                filename = os.path.basename(fpath)\n",
    "\n",
    "                if i % 250 == 0 or i == (len(files) - 1):\n",
    "                    print '%d/%d' % (i, len(files) - 1)\n",
    "                    \n",
    "                p2 = np.percentile(cropped_data, 0.01)\n",
    "                p98 = np.percentile(cropped_data, 99.99)\n",
    "        \n",
    "                cropped_data = exposure.rescale_intensity(cropped_data, in_range=(p2, p98))\n",
    "                data_8bit = exposure.rescale_intensity(cropped_data, in_range='image', out_range=np.uint8).astype(np.uint8)\n",
    "                data_8bit = filters.median(data_8bit, selem=morphology.disk(median_filter_rad))\n",
    "                \n",
    "                th = max_entropy(np.histogram(data_8bit, bins=256, range=(0, 256))[0]) if dsty == 'sparse' else \\\n",
    "                        mh.otsu(data_8bit, ignore_zeros=True)\n",
    "                    \n",
    "                mask = data_8bit >= th \n",
    "                mask = mask.astype(np.uint8)\n",
    "                \n",
    "                io.imsave(os.path.join(output_dir, filename), mask.astype(np.uint8))\n",
    "            \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_features(samples_info, \\\n",
    "                       input_dir_tmpl='./data/pcl_results', \\\n",
    "                       roi_box=((-175,175),(-175,175),(-175,175))):\n",
    "    \n",
    "    roi_side = None\n",
    "    \n",
    "    if roi_box is not None:\n",
    "        roi_side = np.sum(np.abs(roi_box[0]))\n",
    "    \n",
    "    def _create_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        start = timer()\n",
    "        \n",
    "        center = configs['center']\n",
    "        \n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        atype = configs['analysis_type'].strip()\n",
    "        components = atype.lower().split('/')\n",
    "        \n",
    "        if 'polymer' not in atype.lower() and \\\n",
    "           'particle' not in atype.lower():\n",
    "                raise ValueError('The sample component(s) are not specified.')\n",
    "        \n",
    "        for comp in components: \n",
    "            input_dir = os.path.join(input_dir_tmpl, sample_name, comp.lower() + '_masks')\n",
    "            \n",
    "            filenames = os.listdir(input_dir)\n",
    "            files = [os.path.join(input_dir, f) for f in filenames]\n",
    "            \n",
    "            try:\n",
    "                slice_shape = io.imread(files[0]).shape\n",
    "            except:\n",
    "                slice_shape = fb.open(files[0]).data.shape\n",
    "            \n",
    "            print 'Porosity calculation...'\n",
    "            if roi_side is not None:\n",
    "                output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'porosity_stats_{}'.format(roi_side)))\n",
    "            else:\n",
    "                output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'porosity_stats'))\n",
    "                \n",
    "            if not len(files):\n",
    "                raise ValueError('No mask files.')\n",
    "\n",
    "            slice_area = slice_shape[0] * slice_shape[1]\n",
    "                \n",
    "            try:\n",
    "                porosity_comps = [np.count_nonzero(io.imread(fpath)) / float(slice_area) \\\n",
    "                                    for fpath in files]\n",
    "            except:\n",
    "                porosity_comps = [np.count_nonzero(fb.open(fpath).data) / float(slice_area) \\\n",
    "                                    for fpath in files]\n",
    "\n",
    "            pd.DataFrame({'Porosity': [sum(porosity_comps)]}, \\\n",
    "                            index=[sample_name]).to_csv(os.path.join(output_dir, 'porosity.csv'))\n",
    "                \n",
    "            print 'Particles counting...'\n",
    "            if roi_side is not None:\n",
    "                output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'particles_stats_{}'.format(roi_side)))\n",
    "            else:\n",
    "                output_dir = _create_dir(os.path.join(input_dir_tmpl, sample_name, 'particles_stats'))\n",
    "            \n",
    "            depth, height, width  = len(files), slice_shape[0], slice_shape[1]\n",
    "            cd, ch, cw = depth/2, height/2, width/2\n",
    "                \n",
    "            if roi_box is not None:\n",
    "                data_mask_vol = np.zeros(tuple([np.sum(np.abs(v)) for v in roi_box]), dtype=np.uint8)\n",
    "            else:\n",
    "                data_mask_vol = np.zeros((depth, height, width), dtype=np.uint8)\n",
    "            \n",
    "            path_data = itertools.islice(sorted(files), cd+roi_box[0][0], cd+roi_box[0][1]) if roi_box is not None \\\n",
    "                        else sorted(files)\n",
    "            \n",
    "            for i, fpath in enumerate(path_data):\n",
    "                try:\n",
    "                    data_slice = io.imread(fpath)\n",
    "                except:\n",
    "                    data_slice = fb.open(fpath).data\n",
    "                \n",
    "                if roi_box is not None:\n",
    "                    data_slice = data_slice[slice(center[1]+roi_box[1][0], center[1]+roi_box[1][1], 1), \\\n",
    "                                            slice(center[0]+roi_box[2][0], center[0]+roi_box[2][1], 1)]\n",
    "                    \n",
    "                data_mask_vol[i] = data_slice\n",
    "                \n",
    "            print data_mask_vol.shape, data_mask_vol.dtype\n",
    "        \n",
    "            particle_stats, labeled_data = object_counter(data_mask_vol)\n",
    "            particle_stats.to_csv(os.path.join(output_dir, 'particles.csv'))\n",
    "            \n",
    "            write_as_raw(labeled_data.astype(np.uint8), sample_name, output_dir)\n",
    "        \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_porosity(samples_info,\n",
    "                       input_dir='./data/pcl_results',\n",
    "                       median_filter_rad=1,\n",
    "                       only_fibers=False):\n",
    "    def _create_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "\n",
    "        return path\n",
    "        \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        start = timer()\n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        crop_input_dir = os.path.join(input_dir, sample_name, 'cropped')\n",
    "        porosity_output_dir = _create_dir(os.path.join(input_dir, sample_name, 'porosity_masks2'))\n",
    "        \n",
    "        filenames = os.listdir(crop_input_dir)\n",
    "        filenames = [fn for fn in filenames if fn.endswith('.tif')]\n",
    "        files = [os.path.join(crop_input_dir, f) for f in filenames]\n",
    "\n",
    "        material_count = 0\n",
    "        slice_size = fb.open(files[0]).data.size\n",
    "        \n",
    "        for i, fpath in enumerate(sorted(files)):\n",
    "            cropped_data = fb.open(fpath).data\n",
    "            filename = os.path.basename(fpath)\n",
    "\n",
    "            if i % 250 == 0 or i == (len(files) - 1):\n",
    "                print 'Slice: {}/{}'.format(i, len(files) - 1)\n",
    "                    \n",
    "            p2 = np.percentile(cropped_data, 0.01)\n",
    "            p98 = np.percentile(cropped_data, 99.99)\n",
    "        \n",
    "            cropped_data = exposure.rescale_intensity(cropped_data, in_range=(p2, p98))\n",
    "            data_8bit = exposure.rescale_intensity(cropped_data, in_range='image', out_range=np.uint8).astype(np.uint8)\n",
    "            data_8bit = filters.median(data_8bit, selem=morphology.disk(median_filter_rad))\n",
    "            \n",
    "            if not only_fibers:\n",
    "                thv_particles = max_entropy(np.histogram(data_8bit, bins=256, range=(0, 256))[0])\n",
    "                mask_particles = data_8bit >= thv_particles\n",
    "                \n",
    "            thv_polymer = mh.otsu(data_8bit, ignore_zeros=True)\n",
    "            mask_polymer = data_8bit >= thv_polymer\n",
    "            \n",
    "            if not only_fibers:\n",
    "                mask = (mask_particles | mask_polymer)\n",
    "                mask = ndi.morphology.binary_closing(mask, iterations=4)\n",
    "                mask = ndi.morphology.binary_fill_holes(mask)\n",
    "            else:\n",
    "                mask = mask_polymer\n",
    "                \n",
    "            material_count += np.count_nonzero(mask)\n",
    "                \n",
    "            io.imsave(os.path.join(porosity_output_dir, filename), mask.astype(np.uint8))\n",
    "        \n",
    "        material_ratio = float(material_count) / (slice_size * len(files))\n",
    "        print 'Mateial / Total volume: {}'.format(material_ratio)\n",
    "        print 'Porosity: {}'.format(1. - material_ratio)\n",
    "            \n",
    "        end = timer()\n",
    "        print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_sample(samples_configs, \\\n",
    "                   path_template='./data/pcl_raw_data/recon/biomaterials/materials/{0}/tomo1/{1}', \\\n",
    "                   in_folder='slices_phase_particles', \\\n",
    "                   max_area=2500):\n",
    "    for sample_name, configs in samples_configs.iteritems():\n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        bbox = configs['bbox']\n",
    "        atype = configs['analysis_type'].strip()\n",
    "        struct_type = configs['structure']\n",
    "        \n",
    "        components = atype.split('/')\n",
    "        \n",
    "        data_output_dir = path_template.format(sample_name, 'cropped_data')\n",
    "        if not os.path.exists(data_output_dir):\n",
    "            os.makedirs(data_output_dir)\n",
    "        else:\n",
    "            shutil.rmtree(data_output_dir)\n",
    "            os.makedirs(data_output_dir)\n",
    "            \n",
    "        input_dir = path_template.format(sample_name, in_folder)\n",
    "        \n",
    "        for comp in components:\n",
    "            output_dir = path_template.format(sample_name, comp.lower() + '_masks')\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            else:\n",
    "                shutil.rmtree(output_dir)\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            filenames = os.listdir(input_dir)\n",
    "            files = [os.path.join(input_dir, f) for f in filenames]\n",
    "\n",
    "            working_files = files[configs['offset'][0]: configs['offset'][1]]\n",
    "\n",
    "            for i, fpath in enumerate(working_files):\n",
    "                data = io.imread(fpath)\n",
    "\n",
    "                filename = os.path.basename(fpath)\n",
    "\n",
    "                if i % 200 == 0 or i == (len(working_files) - 1):\n",
    "                    print '%d/%d' % (i, len(working_files) - 1)\n",
    "\n",
    "                data = data[bbox[1]:(bbox[1] + bbox[3] + 1), bbox[0]:(bbox[1] + bbox[2] + 1)]\n",
    "                p2 = np.percentile(data, 0.01)\n",
    "                p98 = np.percentile(data, 99.99)\n",
    "                data8bit = exposure.rescale_intensity(data, in_range=(p2, p98), out_range=np.uint8).astype(np.uint8)\n",
    "                hist = np.histogram(data8bit, bins=256, range=(0, 256))[0]\n",
    "            \n",
    "                if 'particle' in comp.lower():\n",
    "                    th = max_entropy(hist)\n",
    "                elif 'polymer' in comp.lower():\n",
    "                    th = filters.threshold_otsu(data8bit, nbins=256)\n",
    "                else:\n",
    "                    raise ValueError('Unknown sample type.')\n",
    "\n",
    "                mask = data8bit <= th\n",
    "                mask = mask.astype(np.uint8)\n",
    "                \n",
    "                if 'polymer' in comp.lower() and struct_type == 'Thick':\n",
    "                    mask = eliminate_structure_holes(mask, max_area=max_area)\n",
    "\n",
    "                io.imsave(os.path.join(output_dir, filename), mask)\n",
    "                \n",
    "                cropped_data_path = os.path.join(data_output_dir, filename)\n",
    "                \n",
    "                if not os.path.isfile(cropped_data_path):\n",
    "                    io.imsave(cropped_data_path, data8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slices2raw(samples_info,\n",
    "               median_rad=4,\n",
    "               roi=((-250,250), (-250,250)),\n",
    "               slice_folder='porosity_masks',\n",
    "               input_dir='./data/pcl_results'):  \n",
    "    for sample_name, configs in samples_info.iteritems():\n",
    "        print 'Sample #' + sample_name\n",
    "        \n",
    "        masks_input_dir = os.path.join(input_dir, sample_name, slice_folder)\n",
    "        files = [os.path.join(masks_input_dir, f) for f in os.listdir(masks_input_dir) if f.endswith('.tif')]\n",
    "\n",
    "        slice_size, depth = (np.sum(np.abs(roi[0])), np.sum(np.abs(roi[1]))), len(files)\n",
    "        \n",
    "        output = np.zeros((depth,) + slice_size, dtype=np.uint8)\n",
    "        cy, cx = fb.open(files[0]).data.shape[0]/2, fb.open(files[0]).data.shape[1]/2\n",
    "        \n",
    "        for i, fpath in enumerate(sorted(files)):\n",
    "            data = fb.open(fpath).data\n",
    "            \n",
    "            if roi is not None:\n",
    "                data = data[(cy+roi[0][0]):(cy+roi[0][1]), (cx+roi[1][0]):(cx+roi[1][1])]\n",
    "                    \n",
    "            output[i] = data\n",
    "        \n",
    "        output = ndi.filters.median_filter(output, footprint=morphology.ball(median_rad))\n",
    "            \n",
    "        outpath = os.path.join(input_dir, sample_name, \n",
    "                                   '{}_{}_8bit_{}x{}x{}_med{}.raw'.format(sample_name, \n",
    "                                                                    slice_folder,\n",
    "                                                                    slice_size[1],\n",
    "                                                                    slice_size[0],\n",
    "                                                                    depth,\n",
    "                                                                    median_rad))\n",
    "        print outpath\n",
    "        output.astype(np.uint8).tofile(outpath)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #PCLSrro\n",
      "0/295\n",
      "250/295\n",
      "295/295\n",
      "Mateial / Total volume: 0.303050037982\n",
      "Porosity: 0.696949962018\n",
      "150.084975958\n",
      "Sample #PCLHAro\n",
      "0/295\n",
      "250/295\n",
      "295/295\n",
      "Mateial / Total volume: 0.2790204162\n",
      "Porosity: 0.7209795838\n",
      "154.198083878\n",
      "Sample #PCLSiro\n",
      "0/295\n",
      "250/295\n",
      "295/295\n",
      "Mateial / Total volume: 0.152441442108\n",
      "Porosity: 0.847558557892\n",
      "152.10194397\n"
     ]
    }
   ],
   "source": [
    "calculate_porosity(sample_configs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #PCLro\n",
      "Slice: 0/455\n",
      "Slice: 250/455\n",
      "Slice: 455/455\n",
      "Mateial / Total volume: 0.141543695716\n",
      "Porosity: 0.858456304284\n",
      "460.857169867\n"
     ]
    }
   ],
   "source": [
    "calculate_porosity(sample_configs_df, only_fibers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample #PCLSiro\n",
      "Slice: 0/295\n",
      "Slice: 250/295\n",
      "Slice: 295/295\n",
      "Mateial / Total volume: 0.159867893411\n",
      "Porosity: 0.840132106589\n",
      "157.406690836\n"
     ]
    }
   ],
   "source": [
    "calculate_porosity(sample_configs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_otsu(data):\n",
    "    hist = np.histogram(data.ravel(), 256)[0]\n",
    "    size = float(data.size)\n",
    "    \n",
    "    th1, th2 = 0, 0\n",
    "    mt, max_var = 0, 0\n",
    "    \n",
    "    for k in xrange(256):\n",
    "        mt += k * hist[k] / size\n",
    "        \n",
    "    w0k, m0k = 0, 0\n",
    "    \n",
    "    for t1 in xrange(256):\n",
    "        w0k += hist[t1] / size\n",
    "        m0k += t1 * hist[t1] / size\n",
    "        m0 = m0k / w0k\n",
    "        \n",
    "        w1k, m1k = 0, 0\n",
    "        \n",
    "        for t2 in xrange(1, 256):\n",
    "            w1k += hist[t2] / size\n",
    "            m1k += t2 * hist[t2] / size\n",
    "            m1 = m1k / w1k\n",
    "            \n",
    "            w2k = 1. - (w0k + w1k)\n",
    "            m2k = mt - (m0k + m1k)\n",
    "            \n",
    "            if w2k <= 0:\n",
    "                break\n",
    "                \n",
    "            m2 = m2k / w2k\n",
    "            \n",
    "            curr_var = w0k * (m0 - mt) * (m0 - mt) + w1k * (m1 - mt) * (m1 - mt) + w2k * (m2 - mt) * (m2 - mt)\n",
    "            \n",
    "            if max_var < curr_var:\n",
    "                max_var = curr_var\n",
    "                th1, th2 = t1, t2\n",
    "    \n",
    "    print max_var\n",
    "    return th1, th2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
